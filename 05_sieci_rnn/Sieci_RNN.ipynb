{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQrgStzQN8uxYtAI1pvf6T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AleksandraJuras2000/Kurs_Udemi_nn/blob/main/05_sieci_rnn/Sieci_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wektoryzacja tekstu czyli przedstawienie za pomocą wartości numerycznych"
      ],
      "metadata": {
        "id": "rLdjAkce_cWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Często w NLP - Natural Language Processing używamy słowa token. Tokenem może być pojedyńczy znak, a także cały wyraz. Wsystko zależy od kontekstu i potrzeb. Apy podzielić tekst na tokeny (w tym przypadku wyrazy) użyjemy funkcji text_to_word_sequence()"
      ],
      "metadata": {
        "id": "0laIUvO3AIJ3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aQgnhrC4-rQy"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence   # ta f pozwoli podzielic tekst i wystadaryzowac(każda litera jst mała, pomijamy znaki interpunkcyjne)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Mam tu tekst, który ma byc podzielony na slowa i zestandaryzowany'\n",
        "\n",
        "tokens = text_to_word_sequence(text)\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yZ_wxTt_10s",
        "outputId": "bc49d77d-5175-4d51-c72f-26c82153e12e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mam',\n",
              " 'tu',\n",
              " 'tekst',\n",
              " 'który',\n",
              " 'ma',\n",
              " 'byc',\n",
              " 'podzielony',\n",
              " 'na',\n",
              " 'slowa',\n",
              " 'i',\n",
              " 'zestandaryzowany']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "teraz taką liste poddajemy wektoryzacji\n",
        "\n",
        "### Kodowanie one_hot()\n",
        "Nazwa sugeruje, że tworzymy kodowanie zero-jednykowe dokumentu, co nie jest prawdą. Polega na przedstawieniu każdego słowa jako unikalnej liczby całkowitej. one_hot(text, n) koduje tekst do listy indeksów słów o rozmiarze n. Jest to opakowanie funkcji hashing_trick używającej hash jako funkcji hashującej.\n",
        "\n",
        "Jednoznaczność mapowania słów na indeksy nie jest gwarantowana. Zastosowanie funkcji hashującej może powodować kolizje i nie wszystkim słowom zostaną przypisane unikalne wartości całkowite.\n",
        "\n",
        "Oprócz tekstu należy podać rozmiar słownika. Może to być łączna liczba słów w dokumencie lub więcej, jeśli zamierzamy zakodować dodatkowe dokumenty zawierające dodatkowe słowa.\n",
        "\n",
        "Rozmiar słownika określa przestrzeń hashująca, z której słowa są hashowane. Najlepiej byłoby, gdyby był on większy niż słownik o pewien procent (np. 25%), aby zminimalizować liczbę kolizji."
      ],
      "metadata": {
        "id": "NdyuXhz5Abwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hash('który')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A0ICBz8Abf_",
        "outputId": "efb01a1e-57bb-4389-f4fe-666448ef86d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "408241773285848098"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hash('który') % 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWMdfaHuBE_T",
        "outputId": "65e75ffb-92e7-480c-d6d2-3f43595a4590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hash('podzielony')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sz0u_PfgBKud",
        "outputId": "3ed8f352-a0ef-43a9-a031-f54a355904a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-7730933916869212197"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hash('który')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKUC-fZBBP2z",
        "outputId": "74d8cb5c-46dc-435c-9b1b-62e69e911ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8253341289964763858"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "\n",
        "words = set(tokens)   # tworzy zbiór\n",
        "one_hot_tokens = one_hot(text, round(len(words) * 1.3))\n",
        "one_hot_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvc9xoiwBRPD",
        "outputId": "990cd084-901e-46f2-a46a-0256860e1395"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 1, 4, 13, 1, 4, 5, 3, 3, 2, 13]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7U_OVC-dGHvv",
        "outputId": "f1e01f90-97fa-491f-a453-540a64cf74bb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mam',\n",
              " 'tu',\n",
              " 'tekst',\n",
              " 'który',\n",
              " 'ma',\n",
              " 'byc',\n",
              " 'podzielony',\n",
              " 'na',\n",
              " 'slowa',\n",
              " 'i',\n",
              " 'zestandaryzowany']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import hashing_trick\n",
        "\n",
        "hashing_tokens = hashing_trick(text, round(len(words) * 1.3), hash_function='md5')\n",
        "hashing_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5VnTvDyGIOx",
        "outputId": "47070011-dc07-4bf7-c8d2-4d560501a984"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6, 3, 4, 4, 13, 3, 9, 3, 11, 3, 6]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizacja"
      ],
      "metadata": {
        "id": "Uz4yP065HPyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "n87kyxy-HWym"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = ['Great picture!', 'Nice view', 'Good to see you :)', 'Good picture!', 'Good']\n",
        "\n",
        "tokenizer.fit_on_texts(samples)\n",
        "tokenizer.index_word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chvel3XgHePt",
        "outputId": "358e75b4-56a8-4a67-ee79-99ecba23d921"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'good',\n",
              " 2: 'picture',\n",
              " 3: 'great',\n",
              " 4: 'nice',\n",
              " 5: 'view',\n",
              " 6: 'to',\n",
              " 7: 'see',\n",
              " 8: 'you'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_counts  # czestosc wystepowania slowa w naszej próbce"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLXoAjx9H47V",
        "outputId": "afaf27e7-491c-4683-e79a-3f9522b9d4a8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('great', 1),\n",
              "             ('picture', 2),\n",
              "             ('nice', 1),\n",
              "             ('view', 1),\n",
              "             ('good', 3),\n",
              "             ('to', 1),\n",
              "             ('see', 1),\n",
              "             ('you', 1)])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.document_count   # 5 komentarzy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX1QNoCdH6Wn",
        "outputId": "a6719ce9-8fc8-44fb-bf97-4a75176c9411"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.index_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsUqYBxTIFy8",
        "outputId": "ca47414c-c482-46bb-e3ce-6952b8d79e10"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'good', 2: 'picture', 3: 'great', 4: 'nice', 5: 'view', 6: 'to', 7: 'see', 8: 'you'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.texts_to_matrix(samples)  # zrozumiałe slowa maja swoje numery mamy 5 wierszy bo 5  komentarzy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfARy_0gIVZh",
        "outputId": "7c8a6e07-293e-4d65-f71d-26084af7cde2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 1., 1., 1.],\n",
              "       [0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Osadzanie słów w przestrzeni\n",
        "kodowanie 01 tworzy za duzo tych wektorów i za dużo danych dlatego wymyslono osadzenia słów"
      ],
      "metadata": {
        "id": "-BU5z3FiJCBu"
      }
    }
  ]
}